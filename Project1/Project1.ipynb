{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP30027 Machine learning Project 1: Gaining Information about Naive Bayes\n",
    "# Author: Jordan Ung <jordanu@student.unimelb.edu.au> [729938]\n",
    "# Last Modified: 02.04.19\n",
    "\n",
    "#### GENERAL GUIDE\n",
    "# Read In And Inspect The Data\n",
    "# Check for missing value - (1) delete rows with missing values, (2) Impute the missing values with dataset\n",
    "# Check for anomalously extreme values\n",
    "\n",
    "# TODO\n",
    "# Fix Info_Gain(), there should not be a case where IG < 0\n",
    "# Note, entropy with n unique values has max value of log2(n)\n",
    "\n",
    "# Work on Question 1, and Question 4\n",
    "# Implement a Cross Evaluation Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess takes the name of a file and returns a list of instances\n",
    "# within that file, with each instance containing a list of attributes\n",
    "def preprocess(file_name):\n",
    "    dataset = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Add each instance to a list to be used later\n",
    "        for line in file.readlines():\n",
    "            dataset.append(line.strip().split(','))\n",
    "            \n",
    "    # Group all the instances with the same class together\n",
    "    dataset = sorted(dataset, key=lambda x: x[-1])\n",
    "    return dataset\n",
    "\n",
    "data_file = 'anneal.csv'\n",
    "new_file = \"_unit_test.csv\"\n",
    "missing_value_files = ['breast-cancer.csv', 'hepatitis.csv', 'hypothyroid.csv', 'mushroom.csv', 'primary-tumor.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train takes a list of instances and returns a 3-tuple containing:\n",
    "# A dictionary of the class distribution of all classes in the dataset\n",
    "# A list of dictionaries, tallying each attribute value for every attribute\n",
    "# A dictionary (for each class) of lists of dictionaries tallying \n",
    "# attribute values for every attribute of a particular class\n",
    "def train(instance_list, missing_value):\n",
    "    data_info = ({}, [], {})\n",
    "\n",
    "    current_class = instance_list[0][-1]\n",
    "    data_info[0][current_class] = 0\n",
    "    data_info[2][current_class] = []\n",
    "    class_index = 0\n",
    "    \n",
    "    # Add attribute lists to store the unique attribute values in\n",
    "    for i in range(len(instance_list[0]) - 1):\n",
    "        data_info[1].append({})\n",
    "        data_info[2][current_class].append({})\n",
    "    \n",
    "    # Tally each value in each attribute for each class\n",
    "    for data in instance_list:\n",
    "#         print(data)\n",
    "        attribute_num = 0\n",
    "\n",
    "        # New class has been detected\n",
    "        if data[-1] != current_class:\n",
    "\n",
    "            # Add data structure to support new class\n",
    "            current_class = data[-1]\n",
    "            data_info[0][current_class] = 0\n",
    "            data_info[2][current_class] = []\n",
    "            class_index += 1\n",
    "            \n",
    "            # Add dictionary for each attribute\n",
    "            for i in range(len(data) - 1):\n",
    "                data_info[2][current_class].append({})\n",
    "        \n",
    "        # Input each instance's attribute into the appropriate dictionary\n",
    "        for attribute in data[:-1]:\n",
    "            \n",
    "            if attribute in data_info[1][attribute_num]:\n",
    "                data_info[1][attribute_num][attribute] += 1\n",
    "            else:\n",
    "                data_info[1][attribute_num][attribute] = 1\n",
    "                \n",
    "            if attribute in data_info[2][current_class][attribute_num]:\n",
    "                data_info[2][current_class][attribute_num][attribute] += 1\n",
    "            else:\n",
    "                data_info[2][current_class][attribute_num][attribute] = 1\n",
    "            attribute_num += 1\n",
    "        \n",
    "        data_info[0][current_class] += 1\n",
    "        \n",
    "    return data_info # Return the 3-tuple\n",
    "\n",
    "\n",
    "# train(preprocess(missing_value_files[0]), \"?\")\n",
    "# train(preprocess(new_file), \"?\")\n",
    "\n",
    "# train(preprocess(missing_value_files[-1]), \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function takes two arguments, a learner model and a\n",
    "# dataset and attempts to predict the class of a certain instance\n",
    "def predict(model, instances):\n",
    "    predicted_classes = []\n",
    "    possible_classes = list(model[0].keys())\n",
    "    \n",
    "    # Find class prediction for each instance\n",
    "    for data in instances:\n",
    "        probability_of_class = 1.0\n",
    "        class_probabilities = []\n",
    "        \n",
    "        # Calculate probability of the instance belonging to a particular class\n",
    "        for class_name in possible_classes:\n",
    "            attribute_list = model[2][class_name]\n",
    "            \n",
    "            # Multiply all of the attribute probabilities\n",
    "            for attribute in range(len(attribute_list)):\n",
    "                if data[attribute] in attribute_list[attribute]:\n",
    "                    probability_of_class *= (attribute_list[attribute][data[attribute]] + 1) / (len(model[1][attribute].keys()) + model[0][class_name])\n",
    "                else:\n",
    "                    probability_of_class /= (len(model[1][attribute].keys()) + model[0][class_name])\n",
    "            probability_of_class *= model[0][class_name] / sum(model[0].values())\n",
    "            class_probabilities.append(probability_of_class)\n",
    "            probability_of_class = 1.0\n",
    "        \n",
    "        class_index = 0\n",
    "        highest_probability = 0\n",
    "        # Predict the class with the highest probability\n",
    "        for i in range(len(class_probabilities)):\n",
    "            if class_probabilities[i] > highest_probability:\n",
    "                highest_probability = class_probabilities[i]\n",
    "                class_index = i\n",
    "        predicted_classes.append(possible_classes[class_index])\n",
    "        \n",
    "    return predicted_classes # Return a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 216 out of 286\n",
      "Accuracy Rate (%):  75.52447552447552\n",
      "-----------------------------------\n",
      "Correct: 130 out of 155\n",
      "Accuracy Rate (%):  83.87096774193549\n",
      "-----------------------------------\n",
      "Correct: 3011 out of 3163\n",
      "Accuracy Rate (%):  95.19443566234588\n",
      "-----------------------------------\n",
      "Correct: 7772 out of 8124\n",
      "Accuracy Rate (%):  95.66715903495815\n",
      "-----------------------------------\n",
      "Correct: 192 out of 339\n",
      "Accuracy Rate (%):  56.63716814159292\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluates the performance of the predictor model\n",
    "# The metric/s evaluated are as follows: Accuracy\n",
    "def evaluate(predictions, dataset):\n",
    "    tries = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == dataset[i][-1]:\n",
    "            correct += 1\n",
    "        tries += 1\n",
    "        \n",
    "    print(\"Correct:\", correct, \"out of\", tries)\n",
    "    print(\"Accuracy Rate (%): \", correct / tries * 100)\n",
    "    print(\"-----------------------------------\")\n",
    "    return\n",
    "\n",
    "# print(\"Preprocess\")\n",
    "# print(preprocess(new_file))\n",
    "# print(\"---------------\")\n",
    "# print(\"Train\")\n",
    "# print((train(preprocess(new_file), \"?\")))\n",
    "# print(\"---------------\")\n",
    "# print(predict((train(preprocess(new_file), \"?\")), preprocess(new_file)))\n",
    "# print(\"---------------\")\n",
    "\n",
    "\n",
    "for i in missing_value_files:\n",
    "    evaluate(predict(train(preprocess(i), \"?\"), preprocess(i)), preprocess(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(model):\n",
    "    # Calculate Entropy of Root Node, a.k.a class distribution entropy\n",
    "    \n",
    "    mean_info_attributes = []\n",
    "    # Find Mean Info of each attribute\n",
    "    \n",
    "    # Calculate Information Gain of an attribute given the root node\n",
    "    # In other words, which attribute is best to split the instances\n",
    "    return attribute_info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "# def info_gain(model):\n",
    "#     attribute_info_gain = []\n",
    "#     # Calculate the Information Gain of each attribute\n",
    "#     for attribute_num in range(len(model[2][0])):\n",
    "#         attribute_totals = []\n",
    "#         total_value = {}\n",
    "        \n",
    "#         # Find all the unique values for a given attribute\n",
    "#         for class_index in range(len(model[0])):\n",
    "#             data = model[2][class_index][attribute_num]\n",
    "            \n",
    "#             # Adds the class' attribute dictionary to the cumulative dictionary\n",
    "#             total_value = {x: total_value.get(x, 0) + data.get(x, 0) for x in set(total_value).union(data)}\n",
    "\n",
    "#         # Delete the 'missing values' totals\n",
    "#         if \"?\" in total_value:\n",
    "#             del total_value[\"?\"]\n",
    "#         unique_values = set(total_value.keys())\n",
    "            \n",
    "#         mean_info = 0\n",
    "# #         print(\"---------------\")\n",
    "# #         print(total_value)\n",
    "# #         print(sum(total_value.values()))\n",
    "# #         print(\"---------------\")\n",
    "        \n",
    "#         attribute_entropy = 0\n",
    "#         for i in total_value:\n",
    "#             attribute_probability = total_value[i] * 1.0 / sum(total_value.values())\n",
    "#             attribute_entropy -= attribute_probability * math.log2(attribute_probability)\n",
    "        \n",
    "#         # Calculate entropy for all different values within an attribute\n",
    "#         for value in unique_values:\n",
    "#             entropy = 0\n",
    "#             for class_index in range(len(model[0])):\n",
    "#                 data = model[2][class_index][attribute_num]\n",
    "# #                 print(data)\n",
    "#                 if value in data:\n",
    "#                     print(\"---\", data[value], \"---\", total_value[value])\n",
    "#                     data_probability = data[value] / total_value[value]\n",
    "#                     entropy -= (data_probability * math.log2(data_probability))\n",
    "#                     print(\"Entropy ---\", entropy, \"---\")\n",
    "#             print(\"Final Entropy:\", entropy)\n",
    "#             mean_info += total_value[value] * 1.0 / sum(total_value.values()) * entropy\n",
    "            \n",
    "# #         print(\"Mean Info\", mean_info)\n",
    "# #         print(\"Attribute Entropy\", attribute_entropy)\n",
    "# # #         print(\"---------------\")\n",
    "#         print(\"ATTR\", attribute_entropy - mean_info)\n",
    "#         # At this point: Found the Mean Info for an attribute\n",
    "        \n",
    "#         # Find entropy of the attribute itself\n",
    "        \n",
    "\n",
    "#     return attribute_info_gain\n",
    "# info_gain(train(preprocess(missing_value_files[0]), \"?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
