{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP30027 Machine learning Project 1: Gaining Information about Naive Bayes\n",
    "# Author: Jordan Ung <jordanu@student.unimelb.edu.au> [729938]\n",
    "# Last Modified: 04.04.19\n",
    "\n",
    "#### GENERAL GUIDE\n",
    "# Read In And Inspect The Data\n",
    "# Check for missing value - (1) delete rows with missing values, (2) Impute the missing values with dataset\n",
    "# Check for anomalously extreme values\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Work on Question 1, and Question 4\n",
    "# Implement a Cross Evaluation Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess takes the name of a file and returns a list of instances\n",
    "# within that file, with each instance containing a list of attributes\n",
    "def preprocess(file_name):\n",
    "    dataset = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Add each instance to a list to be used later\n",
    "        for line in file.readlines():\n",
    "            dataset.append(line.strip().split(','))\n",
    "            \n",
    "    # Group all the instances with the same class together\n",
    "    dataset = sorted(dataset, key=lambda x: x[-1])\n",
    "    return dataset\n",
    "\n",
    "all_files = ['anneal.csv', 'breast-cancer.csv', 'car.csv', 'cmc.csv', 'hepatitis.csv', 'hypothyroid.csv', 'mushroom.csv', 'nursery.csv', 'primary-tumor.csv']\n",
    "new_file = \"_unit_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train takes a list of instances and returns a 3-tuple containing:\n",
    "# A dictionary of the class distribution of all classes in the dataset\n",
    "# A list of dictionaries, tallying each attribute value for every attribute\n",
    "# A dictionary (for each class) of lists of dictionaries tallying \n",
    "# attribute values for every attribute of a particular class\n",
    "def train(instance_list):\n",
    "    data_info = ({}, [], {})\n",
    "\n",
    "    current_class = instance_list[0][-1]\n",
    "    data_info[0][current_class] = 0\n",
    "    data_info[2][current_class] = []\n",
    "    class_index = 0\n",
    "    \n",
    "    # Add attribute lists to store the unique attribute values in\n",
    "    for i in range(len(instance_list[0]) - 1):\n",
    "        data_info[1].append({})\n",
    "        data_info[2][current_class].append({})\n",
    "    \n",
    "    # Tally each value in each attribute for each class\n",
    "    for data in instance_list:\n",
    "#         print(data)\n",
    "        attribute_num = 0\n",
    "\n",
    "        # New class has been detected\n",
    "        if data[-1] != current_class:\n",
    "\n",
    "            # Add data structure to support new class\n",
    "            current_class = data[-1]\n",
    "            data_info[0][current_class] = 0\n",
    "            data_info[2][current_class] = []\n",
    "            class_index += 1\n",
    "            \n",
    "            # Add dictionary for each attribute\n",
    "            for i in range(len(data) - 1):\n",
    "                data_info[2][current_class].append({})\n",
    "        \n",
    "        # Input each instance's attribute into the appropriate dictionary\n",
    "        for attribute in data[:-1]:\n",
    "            \n",
    "            if attribute in data_info[1][attribute_num]:\n",
    "                data_info[1][attribute_num][attribute] += 1\n",
    "            else:\n",
    "                data_info[1][attribute_num][attribute] = 1\n",
    "                \n",
    "            if attribute in data_info[2][current_class][attribute_num]:\n",
    "                data_info[2][current_class][attribute_num][attribute] += 1\n",
    "            else:\n",
    "                data_info[2][current_class][attribute_num][attribute] = 1\n",
    "            attribute_num += 1\n",
    "        \n",
    "        data_info[0][current_class] += 1\n",
    "        \n",
    "    return data_info # Return the 3-tuple\n",
    "\n",
    "\n",
    "# train(preprocess(missing_value_files[0]), \"?\")\n",
    "# train(preprocess(new_file), \"?\")\n",
    "\n",
    "# train(preprocess(missing_value_files[-1]), \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function takes two arguments, a learner model and a\n",
    "# dataset and attempts to predict the class of a certain instance\n",
    "def predict(model, instances):\n",
    "    predicted_classes = []\n",
    "    possible_classes = list(model[0].keys())\n",
    "    \n",
    "    # Find class prediction for each instance\n",
    "    for data in instances:\n",
    "        probability_of_class = 1.0\n",
    "        class_probabilities = []\n",
    "        \n",
    "        # Calculate probability of the instance belonging to a particular class\n",
    "        for class_name in possible_classes:\n",
    "            attribute_list = model[2][class_name]\n",
    "            \n",
    "            # Multiply all of the attribute probabilities\n",
    "            for attribute in range(len(attribute_list)):\n",
    "                if data[attribute] in attribute_list[attribute]:\n",
    "                    probability_of_class *= (attribute_list[attribute][data[attribute]] + 1) / (len(model[1][attribute].keys()) + model[0][class_name])\n",
    "                else:\n",
    "                    probability_of_class /= (len(model[1][attribute].keys()) + model[0][class_name])\n",
    "            probability_of_class *= model[0][class_name] / sum(model[0].values())\n",
    "            class_probabilities.append(probability_of_class)\n",
    "            probability_of_class = 1.0\n",
    "        \n",
    "        class_index = 0\n",
    "        highest_probability = 0\n",
    "        # Predict the class with the highest probability\n",
    "        for i in range(len(class_probabilities)):\n",
    "            if class_probabilities[i] > highest_probability:\n",
    "                highest_probability = class_probabilities[i]\n",
    "                class_index = i\n",
    "        predicted_classes.append(possible_classes[class_index])\n",
    "        \n",
    "    return predicted_classes # Return a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anneal.csv\n",
      "Correct: 828 out of 898\n",
      "Accuracy Rate (%):  92.2\n",
      "-----------------------------------\n",
      "breast-cancer.csv\n",
      "Correct: 216 out of 286\n",
      "Accuracy Rate (%):  75.52\n",
      "-----------------------------------\n",
      "car.csv\n",
      "Correct: 1506 out of 1728\n",
      "Accuracy Rate (%):  87.15\n",
      "-----------------------------------\n",
      "cmc.csv\n",
      "Correct: 745 out of 1473\n",
      "Accuracy Rate (%):  50.58\n",
      "-----------------------------------\n",
      "hepatitis.csv\n",
      "Correct: 130 out of 155\n",
      "Accuracy Rate (%):  83.87\n",
      "-----------------------------------\n",
      "hypothyroid.csv\n",
      "Correct: 3011 out of 3163\n",
      "Accuracy Rate (%):  95.19\n",
      "-----------------------------------\n",
      "mushroom.csv\n",
      "Correct: 7772 out of 8124\n",
      "Accuracy Rate (%):  95.67\n",
      "-----------------------------------\n",
      "nursery.csv\n",
      "Correct: 11703 out of 12960\n",
      "Accuracy Rate (%):  90.3\n",
      "-----------------------------------\n",
      "primary-tumor.csv\n",
      "Correct: 192 out of 339\n",
      "Accuracy Rate (%):  56.64\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluates the performance of the predictor model\n",
    "# The metric/s evaluated are as follows: Accuracy\n",
    "def evaluate(predictions, dataset):\n",
    "    tries = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == dataset[i][-1]:\n",
    "            correct += 1\n",
    "        tries += 1\n",
    "        \n",
    "    print(\"Correct:\", correct, \"out of\", tries)\n",
    "    print(\"Accuracy Rate (%): \", round(correct / tries * 100, 2))\n",
    "    print(\"-----------------------------------\")\n",
    "    return (correct / tries)\n",
    "\n",
    "\n",
    "for i in all_files:\n",
    "    print(i)\n",
    "    evaluate(predict(train(preprocess(i)), preprocess(i)), preprocess(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Information Gain of an attribute given the root node\n",
    "# In other words, which attribute is best to split the instances\n",
    "def info_gain(model):\n",
    "    info_gain_values = []\n",
    "    print(model)\n",
    "    \n",
    "    # Calculate Entropy of Root Node, a.k.a class distribution entropy\n",
    "    root_entropy = 0\n",
    "    for class_name in model[0]:\n",
    "        pr_attribute = model[0][class_name] * 1.0 / sum(model[0].values())\n",
    "        root_entropy -= pr_attribute * math.log2(pr_attribute)\n",
    "    \n",
    "    # Traverse each attribute in model\n",
    "    for attribute_index in range(len(model[1])):\n",
    "        mean_info_list = []\n",
    "        mean_info = 0\n",
    "        \n",
    "        # Calculate entropy of each unique attribute value\n",
    "        for attribute in model[1][attribute_index]:\n",
    "            entropy = 0\n",
    "            attribute_freq = []\n",
    "            # Append each class' attribute's frequency\n",
    "            for class_index in model[0].keys():\n",
    "                if attribute in model[2][class_index][attribute_index]:\n",
    "                    attribute_freq.append(model[2][class_index][attribute_index][attribute])\n",
    "            \n",
    "            # Calculate entropy and add to mean_info\n",
    "            if len(attribute_freq) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                for element in attribute_freq:\n",
    "                    probability = element * 1.0 / sum(attribute_freq)\n",
    "                    entropy -= probability * math.log2(probability)\n",
    "            \n",
    "            # Calculate Mean Info of an attribute\n",
    "            mean_info = entropy * (model[1][attribute_index][attribute] / sum(model[1][attribute_index].values()))\n",
    "            mean_info_list.append(mean_info)\n",
    "            \n",
    "        # Calculate the IG for an attribute with respect to the root node\n",
    "        for value in mean_info_list:\n",
    "            info_gain_values.append(root_entropy - value)\n",
    "    return info_gain_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'e': 4208, 'p': 3916}, [{'x': 3656, 'b': 452, 's': 32, 'f': 3152, 'k': 828, 'c': 4}, {'s': 2556, 'y': 3244, 'f': 2320, 'g': 4}, {'y': 1072, 'w': 1040, 'g': 1840, 'n': 2284, 'e': 1500, 'b': 168, 'u': 16, 'c': 44, 'p': 144, 'r': 16}, {'t': 3376, 'f': 4748}, {'a': 400, 'l': 400, 'n': 3528, 'p': 256, 'f': 2160, 'c': 192, 'y': 576, 's': 576, 'm': 36}, {'f': 7914, 'a': 210}, {'c': 6812, 'w': 1312}, {'b': 5612, 'n': 2512}, {'k': 408, 'n': 1048, 'g': 752, 'w': 1202, 'p': 1492, 'h': 732, 'u': 492, 'e': 96, 'y': 86, 'o': 64, 'b': 1728, 'r': 24}, {'e': 3516, 't': 4608}, {'c': 556, 'e': 1120, 'b': 3776, 'r': 192, '?': 2480}, {'s': 5176, 'f': 552, 'k': 2372, 'y': 24}, {'s': 4936, 'f': 600, 'y': 284, 'k': 2304}, {'w': 4464, 'g': 576, 'p': 1872, 'e': 96, 'o': 192, 'n': 448, 'b': 432, 'c': 36, 'y': 8}, {'w': 4384, 'p': 1872, 'g': 576, 'n': 512, 'e': 96, 'o': 192, 'b': 432, 'y': 24, 'c': 36}, {'p': 8124}, {'w': 7924, 'n': 96, 'o': 96, 'y': 8}, {'o': 7488, 't': 600, 'n': 36}, {'p': 3968, 'e': 2776, 'f': 48, 'l': 1296, 'n': 36}, {'n': 1968, 'k': 1872, 'u': 48, 'w': 2388, 'h': 1632, 'o': 48, 'y': 48, 'b': 48, 'r': 72}, {'n': 400, 'a': 384, 's': 1248, 'y': 1712, 'v': 4040, 'c': 340}, {'g': 2148, 'm': 292, 'u': 368, 'd': 3148, 'p': 1144, 'w': 192, 'l': 832}], {'e': [{'x': 1948, 'b': 404, 's': 32, 'f': 1596, 'k': 228}, {'s': 1144, 'y': 1504, 'f': 1560}, {'y': 400, 'w': 720, 'g': 1032, 'n': 1264, 'e': 624, 'b': 48, 'u': 16, 'c': 32, 'p': 56, 'r': 16}, {'t': 2752, 'f': 1456}, {'a': 400, 'l': 400, 'n': 3408}, {'f': 4016, 'a': 192}, {'c': 3008, 'w': 1200}, {'b': 3920, 'n': 288}, {'k': 344, 'n': 936, 'g': 248, 'w': 956, 'p': 852, 'h': 204, 'u': 444, 'e': 96, 'y': 64, 'o': 64}, {'e': 1616, 't': 2592}, {'c': 512, 'e': 864, 'b': 1920, 'r': 192, '?': 720}, {'s': 3640, 'f': 408, 'k': 144, 'y': 16}, {'s': 3400, 'f': 456, 'y': 208, 'k': 144}, {'w': 2752, 'g': 576, 'p': 576, 'e': 96, 'o': 192, 'n': 16}, {'w': 2704, 'p': 576, 'g': 576, 'n': 64, 'e': 96, 'o': 192}, {'p': 4208}, {'w': 4016, 'n': 96, 'o': 96}, {'o': 3680, 't': 528}, {'p': 3152, 'e': 1008, 'f': 48}, {'n': 1744, 'k': 1648, 'u': 48, 'w': 576, 'h': 48, 'o': 48, 'y': 48, 'b': 48}, {'n': 400, 'a': 384, 's': 880, 'y': 1064, 'v': 1192, 'c': 288}, {'g': 1408, 'm': 256, 'u': 96, 'd': 1880, 'p': 136, 'w': 192, 'l': 240}], 'p': [{'x': 1708, 'f': 1556, 'b': 48, 'k': 600, 'c': 4}, {'s': 1412, 'y': 1740, 'f': 760, 'g': 4}, {'n': 1020, 'w': 320, 'g': 808, 'p': 88, 'y': 672, 'e': 876, 'b': 120, 'c': 12}, {'t': 624, 'f': 3292}, {'p': 256, 'f': 2160, 'c': 192, 'y': 576, 'n': 120, 's': 576, 'm': 36}, {'f': 3898, 'a': 18}, {'c': 3804, 'w': 112}, {'n': 2224, 'b': 1692}, {'k': 64, 'n': 112, 'p': 640, 'w': 246, 'h': 528, 'g': 504, 'u': 48, 'b': 1728, 'r': 24, 'y': 22}, {'e': 1900, 't': 2016}, {'e': 256, 'b': 1856, '?': 1760, 'c': 44}, {'s': 1536, 'k': 2228, 'f': 144, 'y': 8}, {'s': 1536, 'k': 2160, 'f': 144, 'y': 76}, {'w': 1712, 'n': 432, 'b': 432, 'p': 1296, 'c': 36, 'y': 8}, {'w': 1680, 'b': 432, 'p': 1296, 'n': 448, 'y': 24, 'c': 36}, {'p': 3916}, {'w': 3908, 'y': 8}, {'o': 3808, 't': 72, 'n': 36}, {'p': 816, 'l': 1296, 'e': 1768, 'n': 36}, {'k': 224, 'n': 224, 'h': 1584, 'w': 1812, 'r': 72}, {'s': 368, 'v': 2848, 'y': 648, 'c': 52}, {'u': 272, 'g': 740, 'd': 1268, 'p': 1008, 'l': 592, 'm': 36}]})\n",
      "[0.55044319862971, 0.9718983680756454, 0.6111267565576992, 0.9125320692896992, 0.6869442241980224, 0.6012830262081484, 0.7384987761125226, 0.8733086861199351, 0.8850709365292463, 0.775005823007689, 0.720244530417901, 0.818206778052397, 0.981219022485558, 0.9944894329124176, 0.981979351558282, 0.7121083607369063, 0.4793390218967736, 0.9060749773839998, 0.025073460109486523, 0.98815946401359, 0.1688425697909347, 0.9311085110780959, 0.3890199555349614, 0.840202316485545, 0.9675914734471959, 0.9358056889932469, 0.9143976543759462, 0.8908895829296002, 0.8180983130885499, 0.9221504403435988, 0.9711360539462117, 0.9903835943990039, 0.5683152552621059, 0.43826941418001863, 0.9717517662454012, 0.8921533009703031, 0.5343685512465146, 0.7337477097824379, 0.4401267585302312, 0.9428041400519919, 0.9026433328865129, 0.9963550583671295, 0.45556442792999186, 0.9403500032694676, 0.9697726938266369, 0.903411038984031, 0.4712976543384185, 0.793873368254675, 0.9868099446140669, 0.48086723386004815, 0.793873368254675, 0.964810758157764, 0.0, 0.02381701612091669, 0.07754874496593867, 0.9599718211496121, 0.6410121920915786, 0.6760772155744192, 0.8751994107134548, 0.87731477233759, 0.7647828395379201, 0.9606115857133314, 0.8646618627255617, 0.7974002248542106, 0.5638603382955281, 0.973239283808933, 0.7534212029668375, 0.9797041018034005, 0.9615587256588146, 0.622205974058748, 0.9249788151422147, 0.9103042693373784]\n",
      "Correct: 7772 out of 8124\n",
      "Accuracy Rate (%):  95.67\n",
      "-----------------------------------\n",
      "({'Long-term': 333, 'No-use': 629, 'Short-term': 511}, [{'3': 410, '4': 577, '2': 334, '1': 152}, {'3': 352, '4': 899, '1': 44, '2': 178}, {'more': 368, '4': 197, '2': 276, '3': 259, '1': 276, '0': 97}, {'Islam': 1253, 'Non-Islam': 220}, {'No': 1104, 'Yes': 369}, {'3': 585, '1': 436, '2': 425, '4': 27}, {'4': 684, '3': 431, '2': 229, '1': 129}, {'Good': 1364, 'Not-good': 109}], {'Long-term': [{'3': 80, '4': 207, '2': 37, '1': 9}, {'3': 50, '4': 257, '1': 10, '2': 16}, {'more': 99, '4': 62, '2': 56, '3': 70, '1': 46}, {'Islam': 257, 'Non-Islam': 76}, {'No': 244, 'Yes': 89}, {'3': 93, '1': 156, '2': 79, '4': 5}, {'4': 204, '3': 90, '2': 30, '1': 9}, {'Good': 323, 'Not-good': 10}], 'No-use': [{'2': 176, '1': 103, '3': 175, '4': 175}, {'3': 161, '2': 99, '4': 338, '1': 31}, {'3': 70, 'more': 150, '0': 95, '1': 143, '2': 114, '4': 57}, {'Islam': 554, 'Non-Islam': 75}, {'No': 459, 'Yes': 170}, {'2': 200, '3': 258, '1': 158, '4': 13}, {'3': 184, '4': 248, '2': 117, '1': 80}, {'Good': 555, 'Not-good': 74}], 'Short-term': [{'3': 155, '2': 121, '4': 195, '1': 40}, {'3': 141, '2': 63, '4': 304, '1': 3}, {'4': 78, 'more': 119, '1': 87, '2': 106, '3': 119, '0': 2}, {'Islam': 442, 'Non-Islam': 69}, {'No': 401, 'Yes': 110}, {'3': 234, '1': 122, '2': 146, '4': 9}, {'4': 232, '1': 40, '2': 82, '3': 157}, {'Good': 486, 'Not-good': 25}]})\n",
      "[1.1173975392456204, 0.919518278745843, 1.2285355726810634, 1.422558698025662, 1.1937713366734346, 0.5771644712576002, 1.505999459592578, 1.380307081455014, 1.148295698638022, 1.3288187425774638, 1.2534732494839165, 1.268993571501698, 1.2678389180807028, 1.5294926532424757, 0.24634939257411848, 1.3025056921100138, 0.38598539568532564, 1.1556315199441434, 0.9546574272751139, 1.0727356653505113, 1.1083989515265746, 1.5117859201573092, 0.8051961076392742, 1.0922879075489638, 1.3198770727025626, 1.432254121912249, 0.10330638543329895, 1.4515146534120689]\n",
      "Correct: 745 out of 1473\n",
      "Accuracy Rate (%):  50.58\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 1: The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on \n",
    "the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, \n",
    "relative to the class distribution - does this help to explain the classifiers' behaviour? \n",
    "Identify any results that are particularly surprising, and explain why they occur.\n",
    "\n",
    "The two files below, mushroom.csv and cmc.csv are the two datasets with the largest difference\n",
    "in accuracies of classes predicted by the classifier, with mushroom.csv having a 95.67% accuracy\n",
    "while only a 50.58% accuracy from cmc.csv. By initially looking at the information gained from each\n",
    "attribute in both datasets, cmc.csv seems to have a much higher Information Gain per attribute on\n",
    "average, but that is not factored in the amount of classes, which favours highly-branching class\n",
    "distributions. By looking at some of the attributes - for example {'Islam': 1253, 'Non-Islam': 220},\n",
    "it's surprising that this dataset has such a low accuracy. Though when looking at the class distributions\n",
    "as a whole, one class has substancially less instances of that class, such that predicting that class relative\n",
    "to the others is much more harder, leading to a more inaccuracte result. That is also why mushroom.csv has such\n",
    "a good accuracy - there are enough instances of each class that the NB implementation is able to reliably\n",
    "build a model that reflects the nature of, in this case both instances. So while there are highly branching\n",
    "attributes within cmc.csv, they actually inflate the Information Gain and don't help build a reliable\n",
    "predictor model. '''\n",
    "print(info_gain(train(preprocess(\"mushroom.csv\"))))\n",
    "evaluate(predict(train(preprocess(\"mushroom.csv\")), preprocess(\"mushroom.csv\")), preprocess(\"mushroom.csv\"))\n",
    "print(info_gain(train(preprocess(\"cmc.csv\"))))\n",
    "evaluate(predict(train(preprocess(\"cmc.csv\")), preprocess(\"cmc.csv\")), preprocess(\"cmc.csv\"))\n",
    "\n",
    "'''\n",
    "Question 4: Evaluating the model on the same data that we use to train the model is considered to be a major\n",
    "mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy.\n",
    "How does your estimate of effectiveness change, compared to testing on the training data?\n",
    "Explain why. (The result might surprise you!)\n",
    "'''\n",
    "\n",
    "def crosseval(file, m):\n",
    "    dataset = preprocess(file)\n",
    "    accuracy = 0\n",
    "    # Split instances into m partitions to be cross evaluated\n",
    "    data_fragments = []\n",
    "    size_of_partitions = math.floor(len(dataset) / m)\n",
    "    remainder = len(dataset) % m\n",
    "    \n",
    "    for i in range(m):\n",
    "        data_fragments.append([])\n",
    "    \n",
    "    # Randomise the ordering of the dataset and partition\n",
    "    list_num = 0\n",
    "    for data_index in range(len(dataset)):\n",
    "        other_index = random.randint(data_index,len(dataset) - 1)\n",
    "        dataset[data_index], dataset[other_index] = dataset[other_index], dataset[data_index]\n",
    "\n",
    "        data_fragments[list_num % m].append(dataset[data_index])\n",
    "        list_num += 1\n",
    "    \n",
    "    # Perform Cross Validation on each of the m folds\n",
    "    for fragment_index in range(m):\n",
    "        new_dataset = []\n",
    "        test_data = data_fragments[fragment_index]\n",
    "        \n",
    "        # Prepare all the training data\n",
    "        for index in range(m):\n",
    "            if index != fragment_index:\n",
    "                new_dataset += data_fragments[index]\n",
    "        \n",
    "        print(\"Partition:\", fragment_index)\n",
    "        accuracy += evaluate(predict(train(new_dataset), test_data), test_data)\n",
    "    print(\"Overall Accuracy for\", file, \"is (%):\", (accuracy / m * 100))\n",
    "    return\n",
    "\n",
    "# crosseval(\"mushroom.csv\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 5, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "m = []\n",
    "t = [10,5]\n",
    "s = [2, 3]\n",
    "\n",
    "print(m+t+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 22, 32, 42]\n",
      "[12, 32, 22, 42]\n"
     ]
    }
   ],
   "source": [
    "a = [12, 22, 32, 42]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
